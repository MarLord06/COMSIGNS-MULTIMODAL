# ðŸ—ºï¸ Roadmap: MÃ©tricas Avanzadas por Clase y AnÃ¡lisis de Cobertura

> **Fecha**: 20 de enero de 2026  
> **Estado**: ðŸ”„ En progreso  
> **MÃ³dulo**: `comsigns.training.metrics` + `comsigns.analysis`

---

## ðŸ“‹ Contexto del Problema

El sistema actual tiene:
- âœ… MÃ©tricas globales (accuracy, top-k, F1 macro)
- âŒ Sin mÃ©tricas por clase individual
- âŒ Sin anÃ¡lisis de cobertura del dataset
- âŒ Sin matriz de confusiÃ³n
- âŒ Sin identificaciÃ³n de clases problemÃ¡ticas

**Problema**: No podemos diagnosticar quÃ© clases estÃ¡n fallando ni por quÃ©.

**SoluciÃ³n**: Implementar mÃ©tricas granulares por clase + anÃ¡lisis de dataset.

---

## ðŸŽ¯ Objetivos

| ID | Objetivo | Prioridad |
|----|----------|-----------|
| O1 | MÃ©tricas por clase (P/R/F1/Support) | Alta |
| O2 | Top-K accuracy por clase | Alta |
| O3 | AnÃ¡lisis de cobertura del dataset | Alta |
| O4 | Matriz de confusiÃ³n exportable | Media |
| O5 | Logging estructurado por clase | Media |
| O6 | IdentificaciÃ³n automÃ¡tica de clases problemÃ¡ticas | Media |

### No-Objetivos

- âŒ Cambiar el flujo principal de entrenamiento
- âŒ Data augmentation o rebalanceo (solo diagnÃ³stico)
- âŒ MÃ©tricas por signer (signer_id=-1)

---

## ðŸ—ï¸ DiseÃ±o Propuesto

### Arquitectura de Archivos

```
comsigns/
â”œâ”€â”€ training/
â”‚   â””â”€â”€ metrics.py              # EXTENDER: agregar per-class metrics
â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ __init__.py            # NUEVO
â”‚   â”œâ”€â”€ coverage.py            # NUEVO: anÃ¡lisis de cobertura
â”‚   â””â”€â”€ confusion.py           # NUEVO: matriz de confusiÃ³n
â””â”€â”€ scripts/
    â””â”€â”€ analyze_dataset.py     # NUEVO: script de anÃ¡lisis
```

### ExtensiÃ³n de MetricsTracker

```python
class MetricsTracker:
    # ... mÃ©todos existentes ...
    
    def compute_per_class(self) -> Dict[str, Dict[str, float]]:
        """Retorna mÃ©tricas por clase."""
        return {
            "yo": {"precision": 0.8, "recall": 0.7, ...},
            "hola": {"precision": 0.6, ...},
            ...
        }
    
    def get_confusion_matrix(self) -> np.ndarray:
        """Retorna matriz de confusiÃ³n [C x C]."""
        ...
    
    def get_worst_classes(self, k: int = 10) -> List[str]:
        """Retorna las K clases con peor F1."""
        ...
```

### Formato de Salida por Clase

```python
{
    "yo": {
        "support": 12,
        "precision": 0.31,
        "recall": 0.25,
        "f1": 0.28,
        "accuracy": 0.25,
        "top5_acc": 0.75
    },
    ...
}
```

### AnÃ¡lisis de Cobertura

```python
{
    "total_classes": 505,
    "total_instances": 2308,
    "distribution": {
        "min": 1,
        "max": 45,
        "mean": 4.57,
        "median": 3,
        "std": 5.2
    },
    "low_support_classes": ["raro", "extraÃ±o", ...],  # < 5 samples
    "high_support_classes": ["yo", "hola", ...],      # > 20 samples
}
```

---

## ðŸ“ Especificaciones de ImplementaciÃ³n

### 1. Per-Class Metrics (sklearn)

```python
from sklearn.metrics import (
    precision_recall_fscore_support,
    confusion_matrix,
    classification_report
)

# Per-class metrics
precision, recall, f1, support = precision_recall_fscore_support(
    y_true, y_pred, 
    average=None,  # Per-class, not macro
    zero_division=0,
    labels=range(num_classes)
)
```

### 2. Top-K Accuracy por Clase

```python
def compute_topk_per_class(logits, labels, k, num_classes):
    """Compute Top-K accuracy for each class."""
    topk_acc = {}
    for c in range(num_classes):
        mask = (labels == c)
        if mask.sum() == 0:
            topk_acc[c] = 0.0
            continue
        class_logits = logits[mask]
        class_labels = labels[mask]
        _, topk_preds = class_logits.topk(k, dim=1)
        correct = topk_preds.eq(class_labels.unsqueeze(1)).any(dim=1)
        topk_acc[c] = correct.float().mean().item()
    return topk_acc
```

### 3. Logging Estructurado

```
=== Per-Class Metrics (Top 5 Best) ===
Class "hola" | Support: 23 | P: 0.85 | R: 0.78 | F1: 0.81 | Top5: 0.95
Class "gracias" | Support: 18 | P: 0.72 | R: 0.67 | F1: 0.69 | Top5: 0.89
...

=== Per-Class Metrics (Top 5 Worst) ===
Class "raro" | Support: 2 | P: 0.00 | R: 0.00 | F1: 0.00 | Top5: 0.50
...

=== Summary ===
Classes with F1 > 0.5: 45/505 (8.9%)
Classes with F1 = 0: 320/505 (63.4%)
Mean F1 (non-zero support): 0.12
```

---

## ðŸ§ª Tests Requeridos

| Test | DescripciÃ³n |
|------|-------------|
| `test_per_class_metrics_shape` | MÃ©tricas para cada clase |
| `test_confusion_matrix_shape` | Matriz [C x C] |
| `test_coverage_analysis` | EstadÃ­sticas correctas |
| `test_worst_classes_ranking` | Ordenamiento por F1 |
| `test_topk_per_class` | Top-K por clase correcto |

---

## âœ… Criterios de AceptaciÃ³n

- [ ] MetricsTracker extendido con `compute_per_class()`
- [ ] Matriz de confusiÃ³n exportable
- [ ] Script de anÃ¡lisis de cobertura
- [ ] Logging estructurado en trainer
- [ ] Tests unitarios pasan
- [ ] No rompe compatibilidad con trainer actual

---

## ðŸ“š Referencias

- sklearn.metrics.classification_report
- sklearn.metrics.confusion_matrix
- Trainer actual: `training/trainer.py`
- MetricsTracker: `training/metrics.py`
