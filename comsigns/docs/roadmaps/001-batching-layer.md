# ğŸ—ºï¸ Roadmap: Capa de Batching para PyTorch

> **Fecha**: 19 de enero de 2026  
> **Estado**: âœ… Completado  
> **MÃ³dulo**: `comsigns.core.data.loaders`

---

## ğŸ“‹ Contexto del Problema

El sistema ComSigns utiliza un encoder multimodal que procesa secuencias temporales de keypoints (manos, cuerpo, rostro). El dataset AEC ya expone instancias `EncoderReadySample` con la estructura:

```python
@dataclass
class EncoderReadySample:
    gloss: str
    hand_keypoints: np.ndarray   # shape: [T, 168]
    body_keypoints: np.ndarray   # shape: [T, 132]
    face_keypoints: np.ndarray   # shape: [T, 1872]
    gloss_id: Optional[int]
```

**Problema**: Las secuencias tienen longitud temporal `T` variable entre muestras, lo que impide crear batches directamente con PyTorch DataLoader.

---

## ğŸ¯ Objetivos

| ID | Objetivo | Prioridad |
|----|----------|-----------|
| O1 | Implementar `collate_fn` para secuencias de longitud variable | Alta |
| O2 | Padding explÃ­cito y eficiente | Alta |
| O3 | Generar mÃ¡scaras temporales para attention/LSTM | Media |
| O4 | Compatibilidad con cualquier dataset que implemente `BaseDataset` | Alta |
| O5 | No modificar el dataset existente | CrÃ­tica |

---

## ğŸ—ï¸ DiseÃ±o Propuesto

### Arquitectura

```
comsigns/core/data/
â”œâ”€â”€ datasets/           # Ya existe - NO MODIFICAR
â”‚   â”œâ”€â”€ base.py
â”‚   â”œâ”€â”€ sample.py
â”‚   â””â”€â”€ aec/
â””â”€â”€ loaders/            # NUEVO
    â”œâ”€â”€ __init__.py
    â””â”€â”€ collate.py      # collate_fn genÃ©rico
```

### Flujo de Datos

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   AECDataset    â”‚â”€â”€â”€â”€â–¶â”‚  DataLoader  â”‚â”€â”€â”€â”€â–¶â”‚  EncoderBatch   â”‚
â”‚ [EncoderReady   â”‚     â”‚  + collate   â”‚     â”‚  {hand, body,   â”‚
â”‚  Sample, ...]   â”‚     â”‚              â”‚     â”‚   face, labels, â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚   lengths,mask} â”‚
                                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Estructura del Batch de Salida

```python
EncoderBatch = {
    "hand":    Tensor[batch, T_max, 168],   # float32
    "body":    Tensor[batch, T_max, 132],   # float32
    "face":    Tensor[batch, T_max, 1872],  # float32
    "labels":  Tensor[batch],               # int64
    "lengths": Tensor[batch],               # int64 (longitudes originales)
    "mask":    Tensor[batch, T_max]         # bool (True=vÃ¡lido)
}
```

---

## ğŸ”§ Decisiones TÃ©cnicas

### 1. Valor de Padding: `0.0`

**RazÃ³n**: Los keypoints normalizados tienen valores en rango [0, 1]. Usar `0.0` como padding:
- No distorsiona gradientes durante backprop
- Es distinguible de valores vÃ¡lidos
- Compatible con mÃ¡scaras de attention

### 2. Inferencia de Dimensiones (no hardcodear)

**RazÃ³n**: Las dimensiones 168, 132, 1872 pueden cambiar si:
- Se agregan/quitan keypoints
- Se usa un subconjunto de landmarks
- Se integra otro dataset con formato diferente

```python
# âœ… Correcto: inferir del primer sample
feature_dim = arrays[0].shape[1]

# âŒ Incorrecto: hardcodear
feature_dim = 168
```

### 3. FunciÃ³n Pura (sin estado)

**RazÃ³n**: 
- FÃ¡cil de testear (input â†’ output determinÃ­stico)
- Sin efectos secundarios
- Reutilizable entre datasets

### 4. Pre-allocaciÃ³n con `np.full()`

**RazÃ³n**: MÃ¡s eficiente que concatenar arrays incrementalmente.

```python
# âœ… Eficiente: pre-allocar
padded = np.full((batch, T_max, dim), pad_value, dtype=np.float32)
for i, arr in enumerate(arrays):
    padded[i, :len(arr)] = arr

# âŒ Ineficiente: concatenar
padded = np.concatenate([...])  # mÃºltiples allocations
```

### 5. MÃ¡scara de Attention Incluida

**RazÃ³n**: 
- LSTMs necesitan `pack_padded_sequence` â†’ requiere lengths
- Transformers necesitan mÃ¡scara de attention â†’ `mask`
- Incluir ambos permite flexibilidad

---

## ğŸ“ Archivos a Crear

| Archivo | PropÃ³sito | Estado |
|---------|-----------|--------|
| `loaders/__init__.py` | Exportar API pÃºblica | âœ… |
| `loaders/collate.py` | ImplementaciÃ³n de `encoder_collate_fn` | âœ… |
| `tests/unit/test_collate.py` | Tests unitarios | âœ… |

---

## ğŸ§ª Plan de Testing

### Tests Unitarios

| Test | DescripciÃ³n | Estado |
|------|-------------|--------|
| `test_basic_padding` | Padding correcto a T_max | âœ… |
| `test_custom_pad_value` | Usar valor de padding personalizado | âœ… |
| `test_dtypes` | Verificar tipos de datos de salida | âœ… |
| `test_labels_correct` | Labels preservados correctamente | âœ… |
| `test_lengths_correct` | Longitudes originales preservadas | âœ… |
| `test_mask_correctness` | MÃ¡scara True/False correcta | âœ… |
| `test_empty_batch_raises` | Error con batch vacÃ­o | âœ… |
| `test_custom_dimensions` | Dimensiones inferidas, no hardcodeadas | âœ… |
| `test_dataloader_iteration` | IntegraciÃ³n con DataLoader real | âœ… |

### Comando para Ejecutar Tests

```bash
python3 -m pytest tests/unit/test_collate.py -v
```

---

## ğŸ“Š Resultados de ImplementaciÃ³n

```
23 passed in 1.23s âœ…
```

---

## ğŸ’¡ Uso

### BÃ¡sico

```python
from torch.utils.data import DataLoader
from comsigns.core.data.loaders import encoder_collate_fn
from comsigns.core.data.datasets.aec import AECDataset

dataset = AECDataset(Path("data/raw/lsp_aec"))

loader = DataLoader(
    dataset,
    batch_size=32,
    shuffle=True,
    collate_fn=encoder_collate_fn
)

for batch in loader:
    # batch["hand"].shape = [32, T_max, 168]
    # batch["mask"].shape = [32, T_max]
    ...
```

### Con ConfiguraciÃ³n Personalizada

```python
from comsigns.core.data.loaders import create_encoder_collate_fn

collate_fn = create_encoder_collate_fn(
    pad_value=-1.0,      # Padding con -1 en lugar de 0
    include_mask=False   # Sin mÃ¡scara de attention
)

loader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn)
```

---

## ğŸ”œ PrÃ³ximos Pasos Sugeridos

1. **Integrar con MultimodalEncoder**: Modificar `forward()` para aceptar mÃ¡scara
2. **Sampler balanceado**: Implementar sampler por clase para datasets desbalanceados
3. **Augmentation temporal**: Data augmentation en el collate (random crop, speed perturbation)

---

## ğŸ“š Referencias

- [PyTorch DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)
- [pack_padded_sequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html)
- Arquitectura ComSigns: `read-cursor.md`
